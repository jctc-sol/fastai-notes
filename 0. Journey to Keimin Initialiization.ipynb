{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is replication of the blog post by James Dellinger titled - *Weight Initialization in Neural Networks: A Journey From the Basics to Kaiming* (https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79) \n",
    "\n",
    "The attempt here is to gain a first-hand understanding of the **challenge in training deep neural networks** and the **importance of proper weight initialization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploding gradients\n",
    "\n",
    "- neural nets are eseentially a sequence of matrix multiplication operations ex. $Y = Sigmoid(ReLu(f_3(ReLu(f_2(ReLu(f_1(X))))))$ and this is only 3 layers deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0033), tensor(1.0014))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the following initializes weights from a normal 0 mean, unit variance distribution\n",
    "w = torch.randn(xdim,nodes)\n",
    "w.mean(), w.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_stats(x): print(x.mean(), x.std())\n",
    "\n",
    "def fwd(dim, nlayers, var_scale=1):\n",
    "    x = torch.randn(dim)\n",
    "    for i in range(nlayers):\n",
    "        w = torch.randn(dim,dim)*var_scale\n",
    "        x = w@x\n",
    "        if torch.isnan(x.mean()) or x.mean()==0:\n",
    "            print('layer %d - mean: %f, std: %f'%(i,x.mean(),x.std())); return\n",
    "    print('layer %d - mean: %f, std: %f'%(i,x.mean(),x.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 27 - mean: nan, std: 79049849710233833933011818076318990336.000000\n"
     ]
    }
   ],
   "source": [
    "fwd(512, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the code snippet above demonstrates, the magnitude of $x$ in the forward pass explodes toward infinite with only 27 layers deep. Similarly, its gradients will also have exploded when performing the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing gradients\n",
    "\n",
    "- conversely, we have the opposite problem when we attempt to initialize the weights too small\n",
    "- below, lets see what happens when we initialize weights from a normal 0 mean, but 0.01 variance distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-2.9085e-05), tensor(0.0100))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the following initializes weights from a normal 0 mean, unit variance distribution\n",
    "w = torch.randn(xdim,nodes) * 0.01\n",
    "w.mean(), w.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 65 - mean: -0.000000, std: 0.000000\n"
     ]
    }
   ],
   "source": [
    "fwd(512, 100, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnitude of Std-dev\n",
    "\n",
    "- the code snippet below demonstrates that the standard deviation of $y = wx$ (where both $w$ and $x$ have unit variance) is equal to $\\sqrt{dim(x)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average mean: 0.014554840754531324; average std-dev: 22.600583814239503\n"
     ]
    }
   ],
   "source": [
    "dim, mean, std, trials = 512, 0, 0, int(1e4)\n",
    "for i in range(trials):\n",
    "    w, x = torch.randn(dim,dim), torch.randn(dim)\n",
    "    y = w@x\n",
    "    mean += y.mean().item()\n",
    "    std += y.std().item()\n",
    "print('average mean: {}; average std-dev: {}'.format(mean/trials, std/trials))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.627416997969522"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt(dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **as a result, it is good ideal to normalize $w$ initialization by $\\sqrt{dim(x)}$ of input $x$**\n",
    "- the code below validates this intuition, as we've made it through the entire forward pass of 100 layers and the output value remained closer to 0 mean and unit variance than before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "- modern neural net architectures typically contain non-linear activation functions after each hidden layer; it is this injection of non-linearity at each stage that allows neural nets its flexibility to act as an universal function approximator.\n",
    "- however, when activations are included in the mix, the vanishing/exploding gradient problems start to re-emerge as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_with_activation(dim, nlayers, var_scale=1, tanh=False, relu=False):\n",
    "    x = torch.randn(dim)\n",
    "    for i in range(nlayers):\n",
    "        w = torch.randn(dim,dim)*var_scale\n",
    "        if tanh: x = torch.tanh(w@x)\n",
    "        elif relu: x = torch.relu(w@x)\n",
    "        else: x = w@x\n",
    "        if torch.isnan(x.mean()) or x.mean()==0:\n",
    "            print('layer %d - mean: %f, std: %f'%(i,x.mean(),x.std())); return\n",
    "    print('layer %d - mean: %f, std: %f'%(i,x.mean(),x.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 99 - mean: -0.005211, std: 0.059854\n"
     ]
    }
   ],
   "source": [
    "fwd_with_activation(512, 100, math.sqrt(1./512), tanh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 99 - mean: 0.000000, std: 0.000000\n"
     ]
    }
   ],
   "source": [
    "fwd_with_activation(512, 100, math.sqrt(1./512), relu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier (Glorot) initialization\n",
    "\n",
    "- Xavier Glorot and Yoshua Bengio (in their paper, _Understanding the difficulty of training deep feedforward neural networks_), they proposed the initialization scheme of drawing from uniform distribution bounded between $\\pm\\frac{\\sqrt{6}}{\\sqrt{n_{i}+n_{i+1}}}$\n",
    "\n",
    "- where $n_{i}$ is the number of input dimensions coming into the layer $i$ (aka *\"fan-in\"*); and \n",
    "- $n_{i+1}$ is the number of output dimensions coming out of layer $i$ (aka. *\"fan-out\"*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_xavier(dim, nlayers, var_scale=1, tanh=False, relu=False):\n",
    "    x = torch.randn(dim)\n",
    "    bound = math.sqrt(6.)/math.sqrt(dim+dim)\n",
    "    for i in range(nlayers):\n",
    "        w = torch.Tensor(dim,dim).uniform_(-bound,bound)*var_scale\n",
    "        if tanh: x = torch.tanh(w@x)\n",
    "        elif relu: x = torch.relu(w@x)\n",
    "        else: x = w@x\n",
    "        if torch.isnan(x.mean()) or x.mean()==0:\n",
    "            print('layer %d - mean: %f, std: %f'%(i,x.mean(),x.std())); return\n",
    "    print('layer %d - mean: %f, std: %f'%(i,x.mean(),x.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Xavier init works fairly decent with vanilla forward matrix muplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 99 - mean: -0.040578, std: 0.909953\n"
     ]
    }
   ],
   "source": [
    "fwd_xavier(512, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- however, it does not work well when activation units are added into the network; as the variance has mostly vanished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 99 - mean: 0.008958, std: 0.096193\n"
     ]
    }
   ],
   "source": [
    "fwd_xavier(512, 100, tanh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 99 - mean: 0.000000, std: 0.000000\n"
     ]
    }
   ],
   "source": [
    "fwd_xavier(512, 100, relu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnitude of Std-dev when including ReLu\n",
    "\n",
    "- Let's examine what happens to the average mean and standard-deviatiion of the output from a single layer with ReLu activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average mean: 9.021990146636963; average std-dev: 13.182903115558624\n"
     ]
    }
   ],
   "source": [
    "dim, mean, std, trials = 512, 0, 0, int(1e4)\n",
    "for i in range(trials):\n",
    "    w, x = torch.randn(dim,dim), torch.randn(dim)\n",
    "    y = torch.relu(w@x)\n",
    "    mean += y.mean().item()\n",
    "    std += y.std().item()\n",
    "print('average mean: {}; average std-dev: {}'.format(mean/trials, std/trials))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaiming initialization\n",
    "\n",
    "- in Kaiming initialization, they discovered that the average standard deviation of the output from a single layer of the network with ReLu activation is very close to $\\sqrt{\\frac{dim_{input}}{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0\n"
     ]
    }
   ],
   "source": [
    "print(math.sqrt(512/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's try to normalize our output activations by this magnitude and observe that the average standard deviation is much closer to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average mean: 0.5632098125249148; average std-dev: 0.8239358275055886\n"
     ]
    }
   ],
   "source": [
    "dim, mean, std, trials = 512, 0, 0, int(1e4)\n",
    "for i in range(trials):\n",
    "    w, x = torch.randn(dim,dim), torch.randn(dim)\n",
    "    y = torch.relu(w@x) * math.sqrt(2/512)\n",
    "    mean += y.mean().item()\n",
    "    std += y.std().item()\n",
    "print('average mean: {}; average std-dev: {}'.format(mean/trials, std/trials))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average mean: 0.689736943590641; average std-dev: 1.0087386183917522\n"
     ]
    }
   ],
   "source": [
    "dim, mean, std, trials = 512, 0, 0, int(1e4)\n",
    "for i in range(trials):\n",
    "    w, x = torch.randn(dim,dim), torch.randn(dim)\n",
    "    y = torch.relu(w@x) * math.sqrt(3/512)\n",
    "    mean += y.mean().item()\n",
    "    std += y.std().item()\n",
    "print('average mean: {}; average std-dev: {}'.format(mean/trials, std/trials))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's now try our simple pass-forward network with Kaiming initialization scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 99 - mean: 0.374733, std: 0.598513\n"
     ]
    }
   ],
   "source": [
    "fwd_with_activation(512, 100, math.sqrt(2./512), relu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 999 - mean: 0.141731, std: 0.212768\n"
     ]
    }
   ],
   "source": [
    "fwd_with_activation(512, 1000, math.sqrt(2./512), relu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- on average, the Kaiming initialization does keep the resulting mean and variance of our output activations (through the 100-layer (or even 1000-layer) network at a healthy level that neither explodes nor vanishess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
