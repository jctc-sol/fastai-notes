{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download mnist dataset\n",
    "mnist_train = torchvision.datasets.MNIST(root='./data', train=True, download=True)\n",
    "mnist_test = torchvision.datasets.MNIST(root='./data', train=False, download=True)\n",
    "x_train, y_train = mnist_train.data, mnist_train.targets\n",
    "x_test, y_test = mnist_test.data, mnist_test.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28]) torch.Size([60000])\n",
      "torch.Size([10000, 28, 28]) torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic dataset object\n",
    "\n",
    "- From https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class, to define a custom dataset object in PyTorch, one simply inherits from `torch.utils.data.Dataset` and overwrites the `__len__` and `__getitem__` methods.\n",
    "- The code below provides a skeleton framework of what a basic dataset object should look like.\n",
    "- **Note:** the `__getitem__` is analogous to a \"getter\" method that allows you to fetch items via `dataset[5]` instead of `dataset.getitem(5)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, x, y): self.x,self.y = x,y\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset(x_train, y_train)\n",
    "type(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 28, 28]) torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "print(ds[:5][0].shape, ds[:5][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic dataloader object\n",
    "\n",
    "- with the dataset object defined previously, we can iterate through our data samples via a `for` loop structure \n",
    "- but this is not very clean code-wise (i.e. batching samples): \n",
    "    - it does not provide functionalities like **shuffling of data**; and \n",
    "    - it does not load data using multiprocessing\n",
    "- the code below shows the skeleton of a very basic dataflow class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicDataLoader():\n",
    "    def __init__(self, ds, bs): self.ds,self.bs = ds,bs\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "bs = 64\n",
    "dl = BasicDataLoader(ds, bs)\n",
    "xb, yb = next(iter(dl))  # wrap in next(iter(dataloader)) to grab one batch\n",
    "print(xb.shape, yb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random sample\n",
    "\n",
    "- below, we add code for a basic random sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler():\n",
    "    def __init__(self, ds, bs, shuffle=False):\n",
    "        self.n, self.bs, self.shuffle = len(ds), bs, shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        # return random permutation of range of numbers up to n if shuffle is True\n",
    "        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n",
    "        # generate a batch of indices from the shuffled indices list\n",
    "        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ds = Dataset(*ds[:10])\n",
    "s = Sampler(small_ds, 3, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1, 7, 6]), tensor([9, 3, 8]), tensor([2, 5, 0]), tensor([4])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[o for o in s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so all the sampler really does is to return the batches of shuffled indices\n",
    "- we also need a way to collect the samples based on these shuffled indices via the collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(b):\n",
    "    xs, ys = zip(*b)\n",
    "    return torch.stack(xs),torch.stack(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lets modify our basic DataLoader to account for the shuffle & collate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicDataLoader():\n",
    "    def __init__(self, ds, sampler, collate_fn=collate):\n",
    "        self.ds, self.sampler, self.collate_fn = ds, sampler, collate_fn\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "s = Sampler(ds, 64, True)\n",
    "dl = BasicDataLoader(ds, sampler=s, collate_fn=collate)\n",
    "x, y = next(iter(dl))\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch DataLoader\n",
    "\n",
    "- the PyTorch dataloader is superior to the previous basic dataloader since it provides various sampler methodologies, as well it provides multi-processing capabilities to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "bs = 64\n",
    "s = RandomSampler(ds)\n",
    "dl = DataLoader(ds, bs, sampler=s, num_workers=os.cpu_count())\n",
    "db = next(iter(dl))\n",
    "print(db[0].shape, db[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation dataset\n",
    "\n",
    "- almost all the time, you also want to have a validation dataset to evaluate your model to ensure it is not overfitting (during the training loop process, which will be covered later)\n",
    "- note that the batch_size in the validattiion dataloader is set to twice the training batch size - this is because the training loop does not require background gradient computation, so it only uses half the memory, therefore we can be more aggressive with the validatiion dataloader batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size=bs*2, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset(x_train, y_train)\n",
    "valid_ds = Dataset(x_valid, y_valid)\n",
    "train_dl,valid_dl = get_dls(train_ds, valid_ds, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
