{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download mnist dataset\n",
    "mnist_train = torchvision.datasets.MNIST(root='./data', train=True, download=True)\n",
    "mnist_valid = torchvision.datasets.MNIST(root='./data', train=False, download=True)\n",
    "x_train, y_train = mnist_train.data, mnist_train.targets\n",
    "x_valid, y_valid = mnist_valid.data, mnist_valid.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset & Dataloader\n",
    "\n",
    "- for details on the evolution of dataset & dataloader, see notebook **1. PyTorch Dataset & DataLoader**\n",
    "- note that we added `self.x[i].view(-1)` below to flatten the input image (2D) into 1-dimensional array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, x, y): self.x,self.y = x,y\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): \n",
    "        return self.x[i].view(-1).float(), self.y[i]\n",
    "\n",
    "    \n",
    "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size=bs*2, **kwargs))\n",
    "\n",
    "\n",
    "# create dataset object around MNIST dataset\n",
    "train_ds = Dataset(x_train, y_train)\n",
    "valid_ds = Dataset(x_valid, y_valid)\n",
    "\n",
    "# create dataloaders\n",
    "bs = 64\n",
    "dl_train, dl_valid = get_dls(train_ds, valid_ds, bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model\n",
    "\n",
    "- for detaiils on construction of model using nn.Module, see notebook **2. PyTorch Dataset & DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=100, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (3): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(784,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,10),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizer & loss function\n",
    "- for optimizer & loss function, we will use native PyTorch functions\n",
    "- in PyTorch, the `torch.nn.functional` modules is often just referred to as `F`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "opt = optim.SGD(model.parameters(), lr=0.1)\n",
    "# loss function\n",
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation metrics\n",
    "- for this toy mnist problem, we will just use accuracy as the metric of evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    return (torch.argmax(out, dim=1)==yb).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt):\n",
    "    for epoch in range(epochs):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        for xb, yb in dl_train:\n",
    "            # forward pass; compute prediction\n",
    "            pred = model(xb)\n",
    "            # compute loss\n",
    "            loss = loss_func(pred, yb)\n",
    "            # backward pass; compute gradients\n",
    "            loss.backward()\n",
    "            # update model parameters\n",
    "            opt.step()\n",
    "            # zero gradients\n",
    "            opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n"
     ]
    }
   ],
   "source": [
    "fit(5, model, loss_func, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7211, grad_fn=<NllLossBackward>) tensor(0.4531)\n"
     ]
    }
   ],
   "source": [
    "xb, yb = next(iter(dl_train))\n",
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "print(loss, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add validation\n",
    "\n",
    "- it is import to evaluate the performance of the model on the validation dataset as we train our model to detect overfitting\n",
    "- below, we modify our basic training loop to account for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        # set to training mode for layers such as batchnorm, dropout\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            # forward pass; compute prediction\n",
    "            pred = model(xb)\n",
    "            # compute loss\n",
    "            loss = loss_func(pred, yb)\n",
    "            # backward pass; compute gradients\n",
    "            loss.backward()\n",
    "            # update model parameters\n",
    "            opt.step()\n",
    "            # zero gradients\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tot_loss, tot_acc = 0., 0.\n",
    "            for xb, yb in valid_dl:\n",
    "                pred = model(xb)\n",
    "                tot_loss += loss_func(pred, yb)\n",
    "                tot_acc += accuracy(pred, yb)\n",
    "        nv = len(valid_dl)\n",
    "        print(epoch, tot_loss/nv, tot_acc/nv)\n",
    "    return tot_loss/nv, tot_acc/nv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(1.7051) tensor(0.5022)\n",
      "1 tensor(1.6714) tensor(0.5475)\n",
      "2 tensor(1.6568) tensor(0.5829)\n",
      "3 tensor(1.6440) tensor(0.6228)\n",
      "4 tensor(1.6048) tensor(0.6376)\n"
     ]
    }
   ],
   "source": [
    "loss, acc = fit(5, model, loss_func, opt, dl_train, dl_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Databunch & Learner\n",
    "\n",
    "- In the code above, `fit(...)` still requires us passing in many objects, this is inconvenient & easy to make a mistake.\n",
    "- In FastAI, databunch & learner classes are wrapper classes around dataloaders and model + optimizer, respectively create to simplify the training code even further\n",
    "- The code below shows the overall concept of classes `DataBunch` and `Learner`\n",
    "- Note that the parameter c is for number of classes (for classification problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size=bs*2, **kwargs))\n",
    "\n",
    "\n",
    "def get_model(data, lr=0.1, nh=50):\n",
    "    m = next(iter(data.train_dl))[0].shape[1]\n",
    "    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,data.c), nn.Sigmoid())\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBunch():\n",
    "    def __init__(self, train_dl, valid_dl, c=None):  \n",
    "        self.train_dl, self.valid_dl, self.c = train_dl, valid_dl, c\n",
    "        \n",
    "    @property\n",
    "    def train_ds(self): return self.train_dl.dataset\n",
    "\n",
    "    @property\n",
    "    def valid_ds(self): return self.valid_dl.dataset\n",
    "    \n",
    "    \n",
    "class Learner():\n",
    "    def __init__(self, model, opt, loss_func, data):\n",
    "        self.model, self.opt, self.loss_func, self.data = model, opt, loss_func, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate DataBunch object\n",
    "data = DataBunch(*get_dls(train_ds, valid_ds, bs=64), c=y_train.max().item()+1)\n",
    "\n",
    "# instantiate Learner object\n",
    "learn = Learner(*get_model(data), loss_func, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- after all these abstractions & wrappers, the basic training loop simplifies to the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, learn):\n",
    "    for epoch in range(epochs):\n",
    "        learn.model.train()\n",
    "        for xb,yb in learn.data.train_dl:\n",
    "            loss = learn.loss_func(learn.model(xb), yb)\n",
    "            loss.backward()\n",
    "            learn.opt.step()\n",
    "            learn.opt.zero_grad()\n",
    "\n",
    "        learn.model.eval()\n",
    "        with torch.no_grad():\n",
    "            tot_loss,tot_acc = 0.,0.\n",
    "            for xb,yb in learn.data.valid_dl:\n",
    "                pred = learn.model(xb)\n",
    "                tot_loss += learn.loss_func(pred, yb)\n",
    "                tot_acc  += accuracy (pred,yb)\n",
    "        nv = len(learn.data.valid_dl)\n",
    "        print(epoch, tot_loss/nv, tot_acc/nv)\n",
    "    return tot_loss/nv, tot_acc/nv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the `fit` function simplifies to a 1-liner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(2.0595) tensor(0.0970)\n",
      "1 tensor(2.0701) tensor(0.0970)\n",
      "2 tensor(1.9802) tensor(0.2095)\n",
      "3 tensor(1.8819) tensor(0.4069)\n",
      "4 tensor(1.8772) tensor(0.3447)\n"
     ]
    }
   ],
   "source": [
    "loss, acc = fit(5, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
