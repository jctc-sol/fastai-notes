{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AvgStats callback\n",
    "A callback that's responsible for managing the training/validation statistics during the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgStatsCallback(Callback):\n",
    "    def __init__(self, metrics):\n",
    "        self.train_stats,self.valid_stats = AvgStats(metrics,True), AvgStats(metrics,False)\n",
    "        \n",
    "    def begin_epoch(self):\n",
    "        # reset stats at the beginning of each epoch\n",
    "        self.train_stats.reset()\n",
    "        self.valid_stats.reset()\n",
    "        \n",
    "    def after_loss(self):\n",
    "        # update train or validatioin stats depending on whether in train/eval mode\n",
    "        stats = self.train_stats if self.in_train else self.valid_stats\n",
    "        with torch.no_grad(): stats.accumulate(self.run)\n",
    "    \n",
    "    def after_epoch(self):\n",
    "        # show stats after epoch\n",
    "        print(self.train_stats)\n",
    "        print(self.valid_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recorder callback\n",
    "\n",
    "The recorder callback is used in the LRF callback for recording the learning rate and the losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recorder(Callback):\n",
    "    def begin_fit(self):\n",
    "        self.lrs = [[] for _ in self.opt.param_groups] # create list of empty lists\n",
    "        self.losses = []\n",
    "\n",
    "    def after_batch(self):\n",
    "        if not self.in_train: return # stop if not in training mode\n",
    "        for pg,lr in zip(self.opt.param_groups,self.lrs): \n",
    "            lr.append(pg['lr'])\n",
    "        self.losses.append(self.loss.detach().cpu())        \n",
    "\n",
    "    def plot_lr  (self, pgid=-1): plt.plot(self.lrs[pgid])\n",
    "    def plot_loss(self, skip_last=0): plt.plot(self.losses[:len(self.losses)-skip_last])\n",
    "        \n",
    "    def plot(self, skip_last=0, pgid=-1):\n",
    "        losses = [o.item() for o in self.losses]\n",
    "        lrs    = self.lrs[pgid]\n",
    "        n = len(losses)-skip_last\n",
    "        plt.xscale('log')\n",
    "        plt.plot(lrs[:n], losses[:n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Param scheduler callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamScheduler(Callback):\n",
    "    _order=1\n",
    "    def __init__(self, pname, sched_funcs): self.pname,self.sched_funcs = pname,sched_funcs\n",
    "        \n",
    "    def begin_fit(self):\n",
    "        if not isinstance(self.sched_funcs, (list,tuple)): # checks sched_funcs is neither list or tuple\n",
    "            self.sched_funcs = [self.sched_funcs] * len(self.opt.param_groups)\n",
    "\n",
    "    def set_param(self):\n",
    "        assert len(self.opt.param_groups)==len(self.sched_funcs)\n",
    "        for pg,f in zip(self.opt.param_groups,self.sched_funcs):\n",
    "            # sets the parameter group to a schedule as determined by f with\n",
    "            # input being training progress in terms of number of epochs\n",
    "            pg[self.pname] = f(self.n_epochs/self.epochs)\n",
    "            \n",
    "    def begin_batch(self): \n",
    "        # update parameter schedules at beginning of batch if in training mode\n",
    "        if self.in_train: self.set_param()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Finder\n",
    "\n",
    "The Learning Rate Finder (LRF) is, in my opinion, one of FastAI's novelty that allows neural net training to be more reliable. The central idea is to go through a limited number of batches in the dataset, with the learning rate increasing at every batch. \n",
    "\n",
    "As a result, the parameter update is taking larger and larger \"steps\" with each batch. The loss value is expected to gradually decrease with respect to the gradual increase of the learning rate in the beginning - until it eventually hit a point where the step being taken is \"too large\", where the symptom is a sudden large jump in loss value.\n",
    "\n",
    "Below is the LRF implemented as a callback. \n",
    "\n",
    "**NOTE:** this implmentation is missing the part where it saves the models' weights initially and loads it back after LRF finishes its routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR_Find(Callback):\n",
    "    # note this _order=1 where its parent class Callback has _order=0\n",
    "    _order=1\n",
    "    def __init__(self, max_iter=100, min_lr=1e-6, max_lr=10):\n",
    "        self.max_iter,self.min_lr,self.max_lr = max_iter,min_lr,max_lr\n",
    "        self.best_loss = 1e9  # init to an wildly large loss value to start\n",
    "\n",
    "    def begin_batch(self): # runs before each batch begins\n",
    "        if not self.in_train: return\n",
    "        # increase lr exponentially, note that n_iter should be coming from the runner object\n",
    "        pos = self.n_iter/self.max_iter\n",
    "        lr = self.min_lr * (self.max_lr/self.min_lr) ** pos\n",
    "        for pg in self.opt.param_groups: pg['lr'] = lr\n",
    "            \n",
    "    def after_step(self): # check conditions on whether to stop training after updating the model params\n",
    "        if self.n_iter>=self.max_iter or self.loss>self.best_loss*10:\n",
    "            raise CancelTrainException()\n",
    "        if self.loss < self.best_loss: self.best_loss = self.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LRF usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = create_learner(get_model, loss_func, data)\n",
    "run = Runner(cb_funcs=[LR_Find, Recorder])\n",
    "run.fit(2, learn)\n",
    "\n",
    "# show result\n",
    "run.recorder.plot(skip_last=5)\n",
    "run.recorder.plot_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuda callback\n",
    "\n",
    "Uses the callback mechanism to automatically handle sticking parameters to a GPU device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device to stick parameters to, i.e. the first GPU device\n",
    "device = torch.device('cuda',0)\n",
    "\n",
    "class CudaCallback(Callback):\n",
    "    def __init__(self,device): self.device=device\n",
    "    def begin_fit(self): self.model.to(self.device)\n",
    "    def begin_batch(self): self.run.xb,self.run.yb = self.xb.to(self.device),self.yb.to(self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbfs.append(CudaCallback)\n",
    "model = get_cnn_model(data)\n",
    "opt = optim.SGD(model.parameters(), lr=0.4)\n",
    "learn = Learner(model, opt, loss_func, data)\n",
    "run = Runner(cb_funcs=cbfs)\n",
    "run.fit(3, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
